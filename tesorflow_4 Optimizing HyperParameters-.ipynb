{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GridSearch and RandomSearch for Optimizing Model HyperParameters\n",
    "Two of the easiest way to look for better parameters is to use a GridSearch or RandomSearch that goes through a grid(dictionary) of values and tries those combined with cross-validation. In regression, we do not need to worry too much about the splits, for a classifier the splits should usually take the classes into account.\n",
    "The Random Search doesn't go through all the list values rather tries some specified number (n_iter) of random combinations. \n",
    "\n",
    "The default values of hyperparameters are good enough for certain tested problems in academia. However these hyperparameters are not ideal for all problems, in fact, it is unlikely that default values are the best one.\n",
    "\n",
    "In Artificial Neural Networks we have several hyperparameters, to name a few, Dense layer connections, Convolution kernel sizes and channels, activation functions, the number of layers or models, skip layers, dropout layers normalization, optimizers, learning rates, epochs, batch sizes, early stoppings. Just these parameters themselves would make for a quite exponentially large number of combinations. We could easily have 10^10 combinations, each with cross-validations and millions of computations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['age', 'sex', 'bmi', 'children', 'smoker', 'region'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "\n",
    "dataset = pd.read_csv(\"insurance.csv\") #read the dataset\n",
    "\n",
    "features = dataset.iloc[:,0:6] #choose first 7 columns as features\n",
    "print(features.columns)\n",
    "labels = dataset.iloc[:,-1] #choose the final column for prediction\n",
    "\n",
    "features = pd.get_dummies(features) #one hot encoding for categorical variables\n",
    "# Expands to 11 columns, could also be cut to 9.\n",
    "features_train, features_test, labels_train, labels_test = train_test_split(features, labels, test_size=0.33, random_state=42)\n",
    "\n",
    "#standardize\n",
    "ct = ColumnTransformer([('standardize', StandardScaler(), ['age', 'bmi', 'children'])], remainder='passthrough')\n",
    "scaled_features_train = ct.fit_transform(features_train) #gives numpy arrays\n",
    "scaled_features_test = ct.transform(features_test) #gives numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 11\n",
      "Number of samples:  1338\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of features:\", features.shape[1]) \n",
    "#print the number of samples in the dataset\n",
    "print(\"Number of samples: \", features.shape[0]) \n",
    "#summary statistics for numeric features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint as sp_randint\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import make_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def design_model():\n",
    "    model = Sequential(name=\"my_model\")\n",
    "    input = tf.keras.Input(shape=(11,)) # Cannot use input_shape with Cross Validation\n",
    "    model.add(input)\n",
    "    model.add(layers.Dense(11, activation = 'relu'))\n",
    "    model.add(layers.Dense(1))\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate = 0.01)\n",
    "    model.compile(loss='mse', metrics=['mae'], optimizer=opt)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_grid_search():\n",
    "  batch_size = [6, 64]\n",
    "  epochs = [10, 50]\n",
    "  model = KerasRegressor(build_fn=design_model)\n",
    "  param_grid = dict(batch_size=batch_size, epochs=epochs) # Dict \n",
    "  grid = GridSearchCV(estimator = model, param_grid=param_grid, scoring = make_scorer(mean_squared_error, greater_is_better=False),return_train_score = True, cv=5)\n",
    "  # An object/Model for testing out models.\n",
    "\n",
    "  grid_result = grid.fit(features_train, labels_train, verbose = 0) # fitting that model.\n",
    "  \n",
    "  return grid_result # Returning the fitted Grid Object, Not the Grid Object itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_randomized_search():\n",
    "  param_grid = {'batch_size': sp_randint(2, 16), 'nb_epoch': sp_randint(10, 100)}\n",
    "  model = KerasRegressor(build_fn=design_model)\n",
    "  grid = RandomizedSearchCV(estimator = model, param_distributions=param_grid, \n",
    "                            scoring = make_scorer(mean_squared_error, greater_is_better=False), \n",
    "                            n_iter = 12, n_jobs=-1) # njobs -1 uses all processors if one has more to spare.\n",
    "  '''In contrast to GridSearchCV, not all parameter values are tried out, but rather a fixed number of parameter settings is sampled from the specified distributions. \n",
    "  The number of parameter settings that are tried is given by n_iter.'''\n",
    "  random_search = grid.fit(features_train, labels_train, verbose = 0)\n",
    "\n",
    "  return random_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best models are stored in the GridSearch object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Reserv3\\Miniconda3\\envs\\p36env\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "-------------- GRID SEARCH COMPLETED--------------------\n"
     ]
    }
   ],
   "source": [
    "grid_search = do_grid_search()\n",
    "print(\"-------------- GRID SEARCH COMPLETED--------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- RANDOMIZED SEARCH COMPLETED--------------------\n"
     ]
    }
   ],
   "source": [
    "random_search = do_randomized_search()\n",
    "print(\"-------------- RANDOMIZED SEARCH COMPLETED--------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Means</th>\n",
       "      <th>Standard Dev</th>\n",
       "      <th>Params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.282525e+08</td>\n",
       "      <td>1.493241e+07</td>\n",
       "      <td>{'batch_size': 6, 'epochs': 10}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-8.981667e+07</td>\n",
       "      <td>1.722415e+07</td>\n",
       "      <td>{'batch_size': 6, 'epochs': 50}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-2.998472e+08</td>\n",
       "      <td>3.179105e+07</td>\n",
       "      <td>{'batch_size': 64, 'epochs': 10}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.428570e+08</td>\n",
       "      <td>1.206057e+07</td>\n",
       "      <td>{'batch_size': 64, 'epochs': 50}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Means  Standard Dev                            Params\n",
       "0 -1.282525e+08  1.493241e+07   {'batch_size': 6, 'epochs': 10}\n",
       "1 -8.981667e+07  1.722415e+07   {'batch_size': 6, 'epochs': 50}\n",
       "2 -2.998472e+08  3.179105e+07  {'batch_size': 64, 'epochs': 10}\n",
       "3 -1.428570e+08  1.206057e+07  {'batch_size': 64, 'epochs': 50}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df approach\n",
    "#Single trailing underscore naming convention is used to avoid conflicts with Python keywords.\n",
    "grid = pd.DataFrame({'Means':grid_search.cv_results_['mean_test_score'],\n",
    "            'Standard Dev': grid_search.cv_results_['std_test_score'],\n",
    "            'Params': grid_search.cv_results_['params']})\n",
    "\n",
    "grid.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mean</th>\n",
       "      <th>Standard Dev</th>\n",
       "      <th>Parameters</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-3.023433e+08</td>\n",
       "      <td>3.523282e+07</td>\n",
       "      <td>{'batch_size': 6, 'nb_epoch': 39}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-2.948474e+08</td>\n",
       "      <td>2.517244e+07</td>\n",
       "      <td>{'batch_size': 6, 'nb_epoch': 91}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-2.447483e+08</td>\n",
       "      <td>2.843843e+07</td>\n",
       "      <td>{'batch_size': 3, 'nb_epoch': 45}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-3.035510e+08</td>\n",
       "      <td>3.437326e+07</td>\n",
       "      <td>{'batch_size': 6, 'nb_epoch': 38}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-3.185468e+08</td>\n",
       "      <td>3.174341e+07</td>\n",
       "      <td>{'batch_size': 13, 'nb_epoch': 85}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Mean  Standard Dev                          Parameters\n",
       "0 -3.023433e+08  3.523282e+07   {'batch_size': 6, 'nb_epoch': 39}\n",
       "1 -2.948474e+08  2.517244e+07   {'batch_size': 6, 'nb_epoch': 91}\n",
       "2 -2.447483e+08  2.843843e+07   {'batch_size': 3, 'nb_epoch': 45}\n",
       "3 -3.035510e+08  3.437326e+07   {'batch_size': 6, 'nb_epoch': 38}\n",
       "4 -3.185468e+08  3.174341e+07  {'batch_size': 13, 'nb_epoch': 85}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random = pd.DataFrame({'Mean':random_search.cv_results_['mean_test_score'],\n",
    "                      'Standard Dev': random_search.cv_results_['std_test_score'],\n",
    "                       'Parameters': random_search.cv_results_['params']\n",
    "                      })\n",
    "random.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "There is an almost infinite number of iterations we can run on our models for optimizations. \n",
    "I have developed or am in the progress of developing three methods for approaching this problem. \n",
    "Method number one can be read about in the computational reducibility tensorflow_3 project.\n",
    "\n",
    "Another method involves moving the continuous calculations off the table and instead of using linear transformations on the inputs on each layer. That is, with enough data, we do not need to compute 1 + 1 = 2. We could instead access the key '1+1' and, it would return 2. I think this will be key for neural networks and robotics in the future.\n",
    "Let's say we have a matrix x,y,z, and activation ReLU. This non-linear setup does not lend itself to a linear transformation. \n",
    "And we always have to do the calculation, right? - Not necessarily.\n",
    "Let's say we have already done this calculation. We can then replace Input1*Weight1.... + bias and use 'Key':Value analogy of 'matrix + activation': Output.\n",
    "So it is a bit difficult if the values are continuous. So we could sort of approximate, it would be (x,y,z)-esq keys and the more computations that over time were made and stored the hashtable the better it would become. \n",
    "If we could accept some error in the calculations we could move from computation to look-up.\n",
    "\n",
    "The third is in undefined space so far.\n",
    "\n",
    "Optimization is a tough nut to crack. One almost has to use some more simple heuristics about it at some point. Lucky perhaps that there is some Generalized error that cannot be reduced further and being close enough to that error will do."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
